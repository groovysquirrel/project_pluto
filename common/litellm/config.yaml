# ==============================================================================
# LITELLM CONFIGURATION
# ==============================================================================
#
# This file configures LiteLLM Proxy - your unified gateway to multiple LLMs.
#
# WHAT IS LITELLM?
# LiteLLM is a proxy that sits between your applications (OpenWebUI, n8n)
# and LLM providers (AWS Bedrock, OpenAI, Anthropic, etc.). Benefits:
#   - One API to access any LLM provider
#   - Track usage and costs across all models
#   - Switch models without changing your applications
#   - Add caching, rate limiting, and more
#
# HOW IT WORKS:
#   Your App → LiteLLM (port 4000) → AWS Bedrock / OpenAI / etc.
#
# ADDING NEW MODELS:
# To add a new model, add an entry to the model_list below.
# See: https://docs.litellm.ai/docs/providers
#
# ==============================================================================

# ------------------------------------------------------------------------------
# MODEL LIST
# ------------------------------------------------------------------------------
# Each model entry has:
#   - model_name: The name your apps will use to request this model
#   - litellm_params: The actual model details and configuration

model_list:

  # ============================================================================
  # AWS BEDROCK MODELS
  # ============================================================================
  # These models run on AWS Bedrock. Prerequisites:
  #   1. AWS account with Bedrock access enabled
  #   2. Models enabled in Bedrock console (Model access page)
  #   3. Valid AWS credentials via 'aws configure'
  #
  # CROSS-REGION INFERENCE:
  # Newer Claude models require cross-region inference profiles.
  # Use the format: us.anthropic.claude-xxx for US region profiles.
  # This routes requests to the optimal region automatically.
  #
  # Model IDs: https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html
  # ============================================================================

  # ---------------------------------------------------------------------------
  # CLAUDE 3.5 SONNET (Recommended for complex tasks)
  # ---------------------------------------------------------------------------
  # Anthropic's most capable balanced model. Great for:
  #   - Complex reasoning and analysis
  #   - Code generation and debugging
  #   - Long document processing
  # Uses cross-region inference profile for better availability.
  
  - model_name: claude-3.5-sonnet
    litellm_params:
      # Cross-region inference profile for Claude 3.5 Sonnet
      model: bedrock/us.anthropic.claude-3-5-sonnet-20241022-v2:0
      aws_region_name: us-east-1

  # ---------------------------------------------------------------------------
  # CLAUDE 3.7 SONNET (Latest Claude model)
  # ---------------------------------------------------------------------------
  # The newest Claude model with improved capabilities
  
  - model_name: claude-3.7-sonnet
    litellm_params:
      model: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0
      aws_region_name: us-east-1

  # ---------------------------------------------------------------------------
  # CLAUDE 3 HAIKU (Fast and efficient) ✓ TESTED WORKING
  # ---------------------------------------------------------------------------
  # Anthropic's fastest model. Great for:
  #   - Quick responses
  #   - Simple tasks
  #   - Cost-sensitive applications
  
  - model_name: claude-3-haiku
    litellm_params:
      model: bedrock/anthropic.claude-3-haiku-20240307-v1:0
      aws_region_name: us-east-1

  # ---------------------------------------------------------------------------
  # META LLAMA 3.1 (Open source, latest)
  # ---------------------------------------------------------------------------
  # Meta's latest open-source LLM, available through Bedrock
  
  - model_name: llama3.1-70b
    litellm_params:
      model: bedrock/us.meta.llama3-1-70b-instruct-v1:0
      aws_region_name: us-east-1

  - model_name: llama3.1-8b
    litellm_params:
      model: bedrock/us.meta.llama3-1-8b-instruct-v1:0
      aws_region_name: us-east-1

  # ---------------------------------------------------------------------------
  # AMAZON NOVA (Amazon's latest models)
  # ---------------------------------------------------------------------------
  # Amazon's newest model family with excellent performance
  
  - model_name: nova-pro
    litellm_params:
      model: bedrock/us.amazon.nova-pro-v1:0
      aws_region_name: us-east-1

  - model_name: nova-lite
    litellm_params:
      model: bedrock/us.amazon.nova-lite-v1:0
      aws_region_name: us-east-1

  # ---------------------------------------------------------------------------
  # AMAZON TITAN EMBEDDINGS (For RAG in AWS deployment)
  # ---------------------------------------------------------------------------
  # Used by OpenWebUI for RAG when deployed to AWS.
  # This model converts text to vectors for semantic search.
  
  - model_name: text-embedding
    litellm_params:
      model: bedrock/amazon.titan-embed-text-v2:0
      aws_region_name: us-east-1

  # ============================================================================
  # WANT TO ADD MORE PROVIDERS?
  # ============================================================================
  # 
  # OPENAI (uncomment and add OPENAI_API_KEY to .env):
  # - model_name: gpt-4-turbo
  #   litellm_params:
  #     model: gpt-4-turbo-preview
  #     api_key: os.environ/OPENAI_API_KEY
  #
  # ANTHROPIC DIRECT (uncomment and add ANTHROPIC_API_KEY to .env):
  # - model_name: claude-direct
  #   litellm_params:
  #     model: claude-3-5-sonnet-20241022
  #     api_key: os.environ/ANTHROPIC_API_KEY
  #
  # See all providers: https://docs.litellm.ai/docs/providers
  # ============================================================================

# ------------------------------------------------------------------------------
# GENERAL SETTINGS
# ------------------------------------------------------------------------------

general_settings:
  # Master key for authenticating API requests to LiteLLM
  # This should match LITELLM_MASTER_KEY in your .env file
  master_key: os.environ/LITELLM_MASTER_KEY
  
  # Enable the admin UI at http://localhost:4000/ui
  # You can view usage, manage models, and more
  enable_admin_ui: true
  
  # Database for storing usage data, API keys, etc.
  database_url: os.environ/DATABASE_URL

# ------------------------------------------------------------------------------
# LITELLM BEHAVIOR SETTINGS
# ------------------------------------------------------------------------------

litellm_settings:
  # Show detailed logs (helpful for debugging, noisy for production)
  set_verbose: false
  
  # Drop unmapped parameters instead of erroring
  # This helps with compatibility across different models
  drop_params: true
  
  # Number of retries for failed requests
  num_retries: 2
  
  # Timeout in seconds
  request_timeout: 300
